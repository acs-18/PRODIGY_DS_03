# -*- coding: utf-8 -*-
"""PRODIGY_DS_03.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qbu1MqvdJOtU5CyQnlx0nS9BiUNbdS8m
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder,StandardScaler
from sklearn.tree import export_text
from sklearn.model_selection import GridSearchCV

data = pd.read_csv("/content/bank.csv",sep=';')

data.head()

"""DATA PRE-PROCESSING"""

data.info()

data.duplicated().sum()

data.isnull().sum()

sns.histplot(x="age",data=data,kde=True,hue="y")
plt.show("Age Distribution")
plt.show()

plt.figure(figsize=(7,3))
sns.countplot(x="education", data= data, hue ="y")
plt.title("Educational Status")
plt.show()

plt.figure(figsize=(8,5))
sns.countplot(x="housing",data=data,hue="y")
plt.title("Housing Loan")
plt.show()

plt.figure(figsize=(8,5))
sns.countplot(x="loan",data=data,hue="y")
plt.title("Personal Loan")
plt.show

cols = data.select_dtypes("object").columns
cols

le = LabelEncoder()

data[cols] = data[cols].apply(le.fit_transform)
data.head()

plt.figure(figsize=(25,15))
sns.heatmap(data.corr(), cmap='bwr', annot=True)
plt.show()

"""Standarsisation

"""

#splitting Input and Output
X = data.drop("y",axis=1)
y = data["y"]

scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X),columns=X.columns)

#train-test split
train_x,test_x,train_y,test_y = train_test_split(X,y,test_size=0.2)

decision_tree = DecisionTreeClassifier()
decision_tree.fit(train_x,train_y)

print("Training Accuracy :",decision_tree.score(train_x,train_y))
print("Testing Accuracy :",decision_tree.score(test_x,test_y))

yperd= decision_tree.predict(test_x)
print(confusion_matrix(test_y,yperd))
print(classification_report(test_y,yperd))

# Applying Grid search cv to find best estimaters to improve model performance
p_grid = {
    'max_depth': [3,5,7,10,None],
    'criterion':['gini','entropy'],
    'min_samples_split':[2,3,5,7,10],
    'criterion':['gini','entropy']
}

gsv = GridSearchCV(decision_tree,p_grid,cv=5)
gsv.fit(train_x,train_y)

gsv.best_estimator_

gsv.best_params_

clf = DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_split=2)
clf.fit(train_x,train_y)

pred_y = clf.predict(test_x)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

c= confusion_matrix(test_y,pred_y)
ConfusionMatrixDisplay(c,display_labels=clf.classes_).plot()
plt.show()

#Report
print(classification_report(test_y,pred_y))

from sklearn.tree import plot_tree
plt.figure(figsize=(15,10))
plot_tree(clf,filled=True,feature_names=X.columns)
plt.show()